{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guyyariv/AI-Beat-Maker/blob/master/drums_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -qU pyfluidsynth pretty_midi"
      ],
      "metadata": {
        "id": "ADEY6zYWRw8S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ctypes.util\n",
        "orig_ctypes_util_find_library = ctypes.util.find_library\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return orig_ctypes_util_find_library(lib)\n",
        "ctypes.util.find_library = proxy_find_library"
      ],
      "metadata": {
        "id": "LB5Ifpyg7ETe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch fastai music21 pebble fluidsynth midi2audio"
      ],
      "metadata": {
        "id": "6KcI_qea7gBi",
        "outputId": "47b27f72-1121-4c6d-da00-976f049bf330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (1.0.61)\n",
            "Requirement already satisfied: music21 in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
            "Requirement already satisfied: pebble in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: fluidsynth in /usr/local/lib/python3.7/dist-packages (0.2)\n",
            "Requirement already satisfied: midi2audio in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from fastai) (2.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai) (0.12.0+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.3.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from fastai) (4.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (21.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.2)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from fastai) (1.3.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.21.6)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.352.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (0.9.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->fastai) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2022.5.18.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2022.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from midi2audio import FluidSynth\n",
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "WiqaZWDyEFAB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install fluidsynth\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2"
      ],
      "metadata": {
        "id": "5DaOgaSAD_Kc",
        "outputId": "eca767ae-7181-469e-ec5f-129a5d91c851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fluidsynth is already the newest version (1.1.9-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 75 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "from midi2audio import FluidSynth"
      ],
      "metadata": {
        "id": "oiXuSX8sEfsm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mido"
      ],
      "metadata": {
        "id": "YBnVNpDQEXg6",
        "outputId": "b54c7e74-60d8-436c-9d9d-f664aca59c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (1.2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zPjy5dDIwUGY"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "from keras.layers import BatchNormalization as BatchNorm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from mido import Message, MidiFile, MidiTrack, bpm2tempo, MetaMessage\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv1EBpFu9V9l",
        "outputId": "6f3fd3fd-f167-46f8-8d59-687f5704ef3b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network():\n",
        "\n",
        "    \"\"\" This function calls all other functions and trains the LSTM\"\"\"\n",
        "\n",
        "    # notes = get_notes()\n",
        "\n",
        "    with open('/content/new_notes', 'rb') as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "\n",
        "    # get amount of pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
        "\n",
        "    model = create_network(network_input, n_vocab)\n",
        "\n",
        "    train(model, network_input, network_output)"
      ],
      "metadata": {
        "id": "L0iFisM3wvqc"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_notes():\n",
        "    notes = []\n",
        "\n",
        "    for file in glob.glob(\"midi_drums/*.midi\"):\n",
        "        input_midi = MidiFile(file)\n",
        "\n",
        "        for t in input_midi.tracks[-1]:\n",
        "            # if 'track' in t.type:\n",
        "            #     notes.append(t.type)\n",
        "            if t.type == 'program_change':\n",
        "                notes.append(f'{t.type},{t.program},{t.time}')\n",
        "            elif t.type == 'control_change':\n",
        "                notes.append(f'{t.type},{t.value},{t.time}')\n",
        "            elif t.type == 'note_on' or t.type == 'note_off':\n",
        "                notes.append(f'{t.type},{t.note},{t.time},{t.velocity}')\n",
        "            else:\n",
        "                continue\n",
        "    with open('data/notes', 'wb') as filepath:\n",
        "        pickle.dump(notes, filepath)\n",
        "    return notes"
      ],
      "metadata": {
        "id": "jqdlMguh5rzd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "\n",
        "    \"\"\" Prepare the sequences which are the inputs for the LSTM \"\"\"\n",
        "\n",
        "    # sequence length should be changed after experimenting with different numbers\n",
        "    sequence_length = 30\n",
        "\n",
        "    # get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "    # create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    network_input = network_input / float(n_vocab)\n",
        "\n",
        "    network_output = np_utils.to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)"
      ],
      "metadata": {
        "id": "_VT-3w0_5w3K"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        512,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        recurrent_dropout=0.3,\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(264))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    model.load_weights('/content/weights/weights-improvement-30-0.2840-bigger.hdf5')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "zQ_B6sHY5zpm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, network_input, network_output):\n",
        "\n",
        "    \"\"\" train the neural network \"\"\"\n",
        "\n",
        "    filepath = \"weights/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor='loss',\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        mode='min'\n",
        "    )\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    # experiment with different epoch sizes and batch sizes\n",
        "    model.fit(network_input, network_output, epochs=30, batch_size=512, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "1QCit0E250qW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_network()"
      ],
      "metadata": {
        "id": "0KacCi3e53gL",
        "outputId": "09cbd260-a41e-4c63-964d-2d16da0c7bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/30\n",
            "126/126 [==============================] - 44s 311ms/step - loss: 0.3624\n",
            "Epoch 2/30\n",
            "126/126 [==============================] - 40s 320ms/step - loss: 0.3522\n",
            "Epoch 3/30\n",
            "126/126 [==============================] - 40s 314ms/step - loss: 0.3391\n",
            "Epoch 4/30\n",
            "126/126 [==============================] - 39s 313ms/step - loss: 0.3418\n",
            "Epoch 5/30\n",
            "126/126 [==============================] - 40s 314ms/step - loss: 0.3452\n",
            "Epoch 6/30\n",
            "126/126 [==============================] - 40s 319ms/step - loss: 0.3300\n",
            "Epoch 7/30\n",
            "126/126 [==============================] - 40s 315ms/step - loss: 0.3312\n",
            "Epoch 8/30\n",
            "126/126 [==============================] - 40s 319ms/step - loss: 0.3352\n",
            "Epoch 9/30\n",
            "126/126 [==============================] - 40s 321ms/step - loss: 0.3208\n",
            "Epoch 10/30\n",
            "126/126 [==============================] - 40s 318ms/step - loss: 0.3193\n",
            "Epoch 11/30\n",
            "126/126 [==============================] - 40s 319ms/step - loss: 0.3185\n",
            "Epoch 12/30\n",
            "126/126 [==============================] - 40s 320ms/step - loss: 0.3309\n",
            "Epoch 13/30\n",
            "126/126 [==============================] - 40s 316ms/step - loss: 0.3414\n",
            "Epoch 14/30\n",
            "126/126 [==============================] - 40s 314ms/step - loss: 0.3264\n",
            "Epoch 15/30\n",
            "126/126 [==============================] - 40s 315ms/step - loss: 0.3121\n",
            "Epoch 16/30\n",
            "126/126 [==============================] - 40s 318ms/step - loss: 0.3147\n",
            "Epoch 17/30\n",
            "126/126 [==============================] - 40s 316ms/step - loss: 0.3070\n",
            "Epoch 18/30\n",
            "126/126 [==============================] - 40s 317ms/step - loss: 0.3148\n",
            "Epoch 19/30\n",
            "126/126 [==============================] - 40s 314ms/step - loss: 0.3135\n",
            "Epoch 20/30\n",
            "126/126 [==============================] - 40s 316ms/step - loss: 0.2996\n",
            "Epoch 21/30\n",
            "126/126 [==============================] - 41s 325ms/step - loss: 0.2926\n",
            "Epoch 22/30\n",
            "126/126 [==============================] - 40s 316ms/step - loss: 0.2982\n",
            "Epoch 23/30\n",
            "126/126 [==============================] - 40s 314ms/step - loss: 0.2964\n",
            "Epoch 24/30\n",
            "126/126 [==============================] - 40s 317ms/step - loss: 0.2942\n",
            "Epoch 25/30\n",
            "126/126 [==============================] - 40s 316ms/step - loss: 0.2963\n",
            "Epoch 26/30\n",
            "126/126 [==============================] - 40s 317ms/step - loss: 0.2920\n",
            "Epoch 27/30\n",
            "126/126 [==============================] - 40s 317ms/step - loss: 0.2966\n",
            "Epoch 28/30\n",
            "126/126 [==============================] - 40s 316ms/step - loss: 0.2845\n",
            "Epoch 29/30\n",
            "126/126 [==============================] - 39s 313ms/step - loss: 0.2905\n",
            "Epoch 30/30\n",
            "126/126 [==============================] - 41s 323ms/step - loss: 0.2840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(tempo=120, index=1):\n",
        "    \"\"\" Generates the midi file \"\"\"\n",
        "    #load the notes used to train the model\n",
        "    with open('/content/new_notes', 'rb') as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    # Get all pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)\n",
        "    model = create_network(normalized_input, n_vocab)\n",
        "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
        "    create_midi(prediction_output, tempo)\n",
        "    create_wav(index=index)"
      ],
      "metadata": {
        "id": "gkmqMbBD7hu7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequences(notes, pitchnames, n_vocab):\n",
        "\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "\n",
        "    # map back from integers to notes\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    sequence_length = 30\n",
        "    network_input = []\n",
        "    output = []\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    normalized_input = normalized_input / float(n_vocab)\n",
        "\n",
        "    return (network_input, normalized_input)"
      ],
      "metadata": {
        "id": "nzLEiBF1Zo19"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
        "\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = np.random.randint(0, len(network_input)-1)\n",
        "\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(800):\n",
        "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = np.argmax(prediction) # numpy array of predictions\n",
        "        result = int_to_note[index] # indexing the note with the highest probability\n",
        "        prediction_output.append(result) # that note is the prediction output\n",
        "\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    return prediction_output"
      ],
      "metadata": {
        "id": "IluKwwKRZx66"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_midi(prediction_output, tempo):\n",
        "    mid = MidiFile()\n",
        "    track = MidiTrack()\n",
        "    track_meta = MidiTrack()\n",
        "    mid.tracks.append(track_meta)\n",
        "    mid.tracks.append(track)\n",
        "    track_meta.append(MetaMessage('set_tempo', tempo=bpm2tempo(tempo)))\n",
        "    track.append(Message('program_change', channel=9, program=12, time=0))\n",
        "\n",
        "    for pattern in prediction_output:\n",
        "        patterns = pattern.split(',')\n",
        "        if patterns[0] == 'program_change':\n",
        "            track.append(Message(patterns[0], channel=9, program=int(patterns[1]), time=int(patterns[2])))\n",
        "        elif patterns[0] == 'control_change':\n",
        "            track.append(Message(patterns[0], channel=9, value=int(patterns[1]), time=int(patterns[2])))\n",
        "        else:\n",
        "            track.append(Message(patterns[0], channel=9, note=int(patterns[1]), time=int(patterns[2]), velocity=int(patterns[3])))\n",
        "\n",
        "    mid.save('/content/new_drums.mid')"
      ],
      "metadata": {
        "id": "hTdqQXXSZ0R7"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_wav(index):\n",
        "  fs = FluidSynth(sound_font='/content/808.sf2')\n",
        "  fs.midi_to_audio(f'/content/new_drums.mid', f'/content/output_{index}.wav')\n",
        "  print(f'/content/output_{index}.wav is ready')"
      ],
      "metadata": {
        "id": "SRXpbOZl7812"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  generate(tempo=105, index=i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-vx7aPlZ3aq",
        "outputId": "3c7b493e-0b66-4e60-e6a7-b94220543054"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_0.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_1.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_2.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_3.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_4.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_5.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_6.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_7.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_8.wav is ready\n",
            "WARNING:tensorflow:Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "/content/output_9.wav is ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0bEXRAqTphlY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}